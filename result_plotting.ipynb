{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.reproduce_utils import Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(methods, metrics, dataset_names, result_df):\n",
    "    headers = [\"metric\", \"dataset\"]\n",
    "    indices = [\n",
    "        \"method\",\n",
    "        \"seed\",\n",
    "    ]\n",
    "    columns = pd.MultiIndex.from_product([metrics, dataset_names], names=headers)\n",
    "    index = pd.MultiIndex.from_product([methods, [1]], names=indices)\n",
    "    df = pd.DataFrame(columns=columns, index=index)\n",
    "    df.sort_index(inplace=True)\n",
    "    for index, row in result_df.iterrows():\n",
    "        for method in methods:\n",
    "            if int(row[\"dataset_id\"]) not in dataset_names:\n",
    "                continue\n",
    "            if \"logistic\" in method:\n",
    "                score_method = \"linear\"\n",
    "            else:\n",
    "                score_method = method\n",
    "            row_id = (method, 1)\n",
    "            col = (\"acc\", row[\"dataset_id\"])\n",
    "            df.loc[row_id, col] = row[f\"score_{score_method}\"]\n",
    "    return Results(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_rank_table(results: Results):\n",
    "    datasets = results.datasets\n",
    "    metrics = sorted(results.metrics, reverse=True)\n",
    "    # print(results.methods)\n",
    "    df = results.df\n",
    "    results_rank = {}\n",
    "    results_score = {}\n",
    "    for metric in metrics:\n",
    "        if \"time\" in metric:\n",
    "            continue\n",
    "        metric_df = df[metric]\n",
    "        dataset_rank_dfs = []\n",
    "        dataset_mean_dfs = []\n",
    "        for dataset in datasets:\n",
    "            if dataset not in metric_df.columns:\n",
    "                continue\n",
    "            dataset_rank_df = metric_df[dataset].groupby('method').mean().rank(ascending=False)\n",
    "            dataset_rank_dfs.append(dataset_rank_df)\n",
    "            dataset_mean_dfs.append(metric_df[dataset])\n",
    "\n",
    "        results_rank[metric.upper()] = pd.concat(dataset_rank_dfs).groupby(\"method\").mean()\n",
    "        \n",
    "        results_score[metric.upper()] = pd.concat(dataset_mean_dfs).groupby(\"method\").mean()\n",
    "    score_df = pd.DataFrame(results_score).reset_index()\n",
    "    rank_df = pd.DataFrame(results_rank).reset_index()\n",
    "    final_table = rank_df.merge(score_df, on=\"method\", suffixes=[\" Mean Rank\", \" Mean Score\"]).T\n",
    "    final_table.columns = final_table.iloc[0]\n",
    "    final_table = final_table.iloc[1:]\n",
    "    return final_table\n",
    "\n",
    "def pprint(df):\n",
    "    for column in df:\n",
    "        df[column] = df[column].astype('float').round(decimals=4)\n",
    "\n",
    "    print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_too_easy_select_acc_to_criteria(df: pd.DataFrame, better_methods: list, worse_methods: list):\n",
    "        \n",
    "    lhs = df[better_methods].max(axis=1) if len(better_methods) > 1 else df[better_methods[0]]\n",
    "    rhs = df[worse_methods].max(axis=1) if len(worse_methods) > 1 else df[worse_methods[0]]\n",
    "    selection_criteria = lhs < 1.05 * rhs\n",
    "    too_easy_on_selection_criteria = df.loc[selection_criteria].index.to_list()\n",
    "    select_on_selection_criteria = df.loc[list(map(lambda x: not x, selection_criteria))].index.to_list()\n",
    "    return too_easy_on_selection_criteria, select_on_selection_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_datasets_info = pd.read_csv(\"csv_files/Datasets tabular data benchmark - numerical_classif-3.csv\")\n",
    "full_datasets_info[\"score_hgbt\"] = full_datasets_info[\"score_hbgt\"].str.replace(',', '.').astype(float)\n",
    "full_datasets_info[\"score_linear\"] = full_datasets_info[\"score_logistic\"].str.replace(',', '.').astype(float)\n",
    "full_datasets_info = full_datasets_info.fillna(0)\n",
    "real_run_dataset_ids = []\n",
    "too_easy_dids = []\n",
    "not_too_easy_dids = []\n",
    "for index, row in full_datasets_info.iterrows():\n",
    "    if int(row[\"dataset_id\"]) != 0 and int(row[\"Remove\"]) != 1 and row[\"Redundant\"] != 1:\n",
    "        prefix_to_skip = [\"BNG\", \"RandomRBF\", \"GTSRB\", \"CovPokElec\", \"PCam\"]\n",
    "        if not (np.any([row[\"dataset_name\"].startswith(prefix) for prefix in\n",
    "                        prefix_to_skip]) or \"mnist\" in row[\"dataset_name\"].lower() or \"image\" in row[\n",
    "                    \"dataset_name\"].lower() or \"cifar\" in row[\"dataset_name\"].lower() or row[\"dataset_id\"] == 1414):\n",
    "                    if row[\"score_hgbt\"] == 0:\n",
    "                        continue\n",
    "                    real_run_dataset_ids.append(int(row[\"dataset_id\"]))\n",
    "                    if int(row[\"too_easy\"]) == 1:\n",
    "                        too_easy_dids.append(int(row[\"dataset_id\"]))\n",
    "                    else:\n",
    "                        not_too_easy_dids.append(int(row[\"dataset_id\"]))\n",
    "        # print(row[\"dataset_id\"])\n",
    "# real_too_easy_dataset_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_run_dataset_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_their_results = full_datasets_info[full_datasets_info[\"dataset_id\"].isin(real_run_dataset_ids)]\n",
    "run_their_results = run_their_results.astype({'dataset_id': int}).set_index(\"dataset_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_easy_hgbt_linear_dids_their, select_hgbt_linear_dids_their = get_too_easy_select_acc_to_criteria(run_their_results, better_methods=[\"score_hgbt\"], worse_methods=[\"score_linear\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TOO EASY (HGBT) VS (LOGREG) ON THEIR (58)\n",
      "|                |   hgbt |   logistic |\n",
      "|:---------------|-------:|-----------:|\n",
      "| ACC Mean Rank  | 1.3707 |     1.6293 |\n",
      "| ACC Mean Score | 0.8391 |     0.8334 |\n",
      "\n",
      "\n",
      "SELECT (HGBT) VS (LOGREG) ON THEIR (13)\n",
      "|                |   hgbt |   logistic |\n",
      "|:---------------|-------:|-----------:|\n",
      "| ACC Mean Rank  | 1      |     2      |\n",
      "| ACC Mean Score | 0.7966 |     0.7149 |\n"
     ]
    }
   ],
   "source": [
    "ranks_df_their = {\n",
    "    \"too_easy_(HGBT)_vs_(Logreg)_on_their\": {\n",
    "        \"ranks\": None,\n",
    "        \"dids\": too_easy_hgbt_linear_dids_their},\n",
    "    \"select_(HGBT)_vs_(Logreg)_on_their\": {\n",
    "        \"ranks\": None,\n",
    "        \"dids\": select_hgbt_linear_dids_their},\n",
    "    }\n",
    "methods = [ \"hgbt\", \"logistic\"] # , \"resnet\", \"rf\", \"tree\", \"mlp\",]\n",
    "metrics = [\"acc\"]\n",
    "for key in ranks_df_their:\n",
    "    current_result = get_result(methods, metrics, ranks_df_their[key]['dids'], run_their_results.reset_index())\n",
    "    current_ranks = get_average_rank_table(current_result)\n",
    "    ranks_df_their[key]['ranks'] = current_ranks\n",
    "    print(f\"\\n\\n{key.upper().replace('_', ' ')} ({len(ranks_df_their[key]['dids'])})\")\n",
    "    pprint(current_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TOO EASY (HGBT) VS (LOGREG) ON THEIR RESULTS (58)\n",
      "|                |   hgbt |   logistic |\n",
      "|:---------------|-------:|-----------:|\n",
      "| ACC Mean Rank  | 1.2931 |     1.7069 |\n",
      "| ACC Mean Score | 0.8444 |     0.8354 |\n",
      "\n",
      "\n",
      "SELECT (HGBT) VS (LOGREG) ON THEIR RESULTS (13)\n",
      "|                |   hgbt |   logistic |\n",
      "|:---------------|-------:|-----------:|\n",
      "| ACC Mean Rank  | 1      |      2     |\n",
      "| ACC Mean Score | 0.8337 |      0.722 |\n"
     ]
    }
   ],
   "source": [
    "ranks_df_their = {\n",
    "    \"too_easy_(HGBT)_vs_(Logreg)_on_their_results\": {\n",
    "        \"ranks\": None,\n",
    "        \"dids\": too_easy_hgbt_linear_dids_their},\n",
    "    \"select_(HGBT)_vs_(Logreg)_on_their_results\": {\n",
    "        \"ranks\": None,\n",
    "        \"dids\": select_hgbt_linear_dids_their},\n",
    "    }\n",
    "methods = [ \"hgbt\", \"logistic\"] # , \"resnet\", \"rf\", \"tree\", \"mlp\",]\n",
    "metrics = [\"acc\"]\n",
    "for key in ranks_df_their:\n",
    "    current_result = get_result(methods, metrics, ranks_df_their[key]['dids'], all_my_results.reset_index())\n",
    "    current_ranks = get_average_rank_table(current_result)\n",
    "    ranks_df_their[key]['ranks'] = current_ranks\n",
    "    print(f\"\\n\\n{key.upper().replace('_', ' ')} ({len(ranks_df_their[key]['dids'])})\")\n",
    "    pprint(current_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_easy_without_resnet = pd.read_csv(\"csv_files/new_too_easy_without_resnet_numerical.csv\", index_col=None)\n",
    "too_easy_resnet = pd.read_csv(\"csv_files/new_too_easy_resnet_numerical_60_0.csv\", index_col=None)\n",
    "too_easy_resnet_2 = pd.read_csv(\"csv_files/new_too_easy_resnet_numerical_60_1.csv\", index_col=None)\n",
    "too_easy_resnet = pd.concat([too_easy_resnet, too_easy_resnet_2])\n",
    "too_easy_hgbt = pd.read_csv(\"csv_files/too_easy_hgbt_numerical_with_preprocessing.csv\")\n",
    "\n",
    "too_easy_results = too_easy_without_resnet.set_index(\"dataset_id\").copy()\n",
    "too_easy_results[\"score_resnet\"] = too_easy_resnet.set_index(\"dataset_id\")[\"score_resnet\"]\n",
    "\n",
    "too_easy_results[\"score_hgbt\"] = too_easy_hgbt.set_index(\"dataset_id\")[\"score_hgbt\"]\n",
    "\n",
    "remaining_resuls_resnet = pd.read_csv(\"csv_files/remaining_resnet_numerical_with_preprocessing.csv\", index_col=None).dropna(subset=\"score_resnet\")\n",
    "not_too_easy_results_resnet = pd.read_csv(\"csv_files/not_too_easy_resnet_with_preprocesssing.csv\", index_col=None).dropna(subset=\"score_resnet\")\n",
    "remaining_resnet_results = pd.concat([remaining_resuls_resnet, not_too_easy_results_resnet]).drop_duplicates(subset='dataset_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_resuls = pd.read_csv(\"csv_files/remaining_without_resnet_numerical_with_preprocessing_.csv\", index_col=None).dropna(subset=\"score_hgbt\")\n",
    "not_too_easy_results = pd.read_csv(\"csv_files/not_too_easy_without_resnet_numerical_with_preprocessing.csv\", index_col=None).dropna(subset=\"score_hgbt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ids = set(remaining_resnet_results['dataset_id'].to_list()) - set(remaining_resuls['dataset_id'].to_list())\n",
    "remaining_resnet_results = remaining_resnet_results[~remaining_resnet_results['dataset_id'].isin(drop_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_resnet_results = remaining_resnet_results.set_index('dataset_id')\n",
    "remaining_resuls = remaining_resuls.set_index('dataset_id')\n",
    "remaining_resuls['score_resnet'] = remaining_resnet_results['score_resnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2836535/661275513.py:4: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  all_my_results = pd.concat([too_easy_results, remaining_resuls])\n",
      "/tmp/ipykernel_2836535/661275513.py:6: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  all_my_results = all_my_results.loc[set(all_my_results.index) & set(run_their_results.index)]\n"
     ]
    }
   ],
   "source": [
    "methods = [ \"hgbt\", \"linear\", \"rf\", \"tree\", \"mlp\",\"resnet\"]\n",
    "\n",
    "subset = [f\"score_{method}\" for method in methods]\n",
    "all_my_results = pd.concat([too_easy_results, remaining_resuls])\n",
    "all_my_results = all_my_results.dropna(subset=subset)\n",
    "all_my_results = all_my_results.loc[set(all_my_results.index) & set(run_their_results.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_my_results.drop(\"Unnamed: 0\", axis=1).to_csv(\"csv_files/all_my_results_remaining.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_easy_linear_dids, select_linear_dids = get_too_easy_select_acc_to_criteria(all_my_results, better_methods=[\"score_hgbt\", \"score_resnet\"], worse_methods=[\"score_linear\"])\n",
    "too_easy_tree_dids, select_tree_dids = get_too_easy_select_acc_to_criteria(all_my_results, better_methods=[\"score_hgbt\", \"score_resnet\"], worse_methods=[\"score_tree\"])\n",
    "too_easy_combined_dids, select_combined_dids = get_too_easy_select_acc_to_criteria(all_my_results, better_methods=[\"score_hgbt\", \"score_resnet\"], worse_methods=[\"score_tree\", \"score_linear\"])\n",
    "too_easy_hgbt_tree_dids, select_hgbt_tree_dids = get_too_easy_select_acc_to_criteria(all_my_results, better_methods=[\"score_hgbt\"], worse_methods=[\"score_tree\"])\n",
    "too_easy_hgbt_linear_dids, select_hgbt_linear_dids = get_too_easy_select_acc_to_criteria(all_my_results, better_methods=[\"score_hgbt\"], worse_methods=[\"score_linear\"])\n",
    "too_easy_hgbt_combined_dids, select_hgbt_combined_dids = get_too_easy_select_acc_to_criteria(all_my_results, better_methods=[\"score_hgbt\"], worse_methods=[\"score_tree\", \"score_linear\"])\n",
    "too_easy_resnet_tree_dids, select_resnet_tree_dids = get_too_easy_select_acc_to_criteria(all_my_results, better_methods=[\"score_resnet\"], worse_methods=[\"score_tree\"])\n",
    "too_easy_mlp_tree_dids, select_mlp_tree_dids = get_too_easy_select_acc_to_criteria(all_my_results, better_methods=[\"score_mlp\"], worse_methods=[\"score_tree\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [ \"hgbt\", \"logistic\", \"resnet\", \"rf\", \"tree\", \"mlp\",]\n",
    "metrics = [\"acc\"]\n",
    "# too_easy_linear_on_my_result = get_result(methods, metrics, too_easy_linear_dids, all_my_results.reset_index())\n",
    "# too_easy_linear_on_my_ranks = get_average_rank_table(too_easy_linear_on_my_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_df = {\n",
    "    # \"too_easy_(HGBT,_Resnet)_vs_(Logreg)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": too_easy_linear_dids},\n",
    "    # \"select_(HGBT,_Resnet)_vs_(Logreg)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": select_linear_dids},\n",
    "    # \"too_easy_(HGBT,_Resnet)_vs_(tree)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": too_easy_tree_dids},\n",
    "    # \"select_(HGBT,_Resnet)_vs_(tree)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": select_tree_dids},\n",
    "    # \"too_easy_(HGBT,_Resnet)_vs_(tree,Logreg)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": too_easy_combined_dids},\n",
    "    # \"select_(HGBT,_Resnet)_vs_(tree,Logreg)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": select_combined_dids},\n",
    "    # \"too_easy_(Resnet)_vs_(tree)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": too_easy_resnet_tree_dids},\n",
    "    # \"select_(Resnet)_vs_(tree)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": select_resnet_tree_dids},\n",
    "    \"too_easy_(MLP)_vs_(tree)_on_my\": {\n",
    "        \"ranks\": None,\n",
    "        \"dids\": too_easy_mlp_tree_dids},\n",
    "    \"select_(MLP)_vs_(tree)_on_my\": {\n",
    "        \"ranks\": None,\n",
    "        \"dids\": select_mlp_tree_dids},\n",
    "    # \"too_easy_(HGBT)_vs_(tree)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": too_easy_hgbt_tree_dids},\n",
    "    # \"select_(HGBT)_vs_(tree)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": select_hgbt_tree_dids},\n",
    "    # \"too_easy_(HGBT)_vs_(Logreg)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": too_easy_hgbt_linear_dids},\n",
    "    # \"select_(HGBT)_vs_(Logreg)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": select_hgbt_linear_dids},\n",
    "    # \"too_easy_(HGBT)_vs_(tree,Logreg)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": too_easy_hgbt_combined_dids},\n",
    "    # \"select_(HGBT)_vs_(tree,Logreg)_on_my\": {\n",
    "    #     \"ranks\": None,\n",
    "    #     \"dids\": select_hgbt_combined_dids},\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TOO EASY (MLP) VS (TREE) ON MY (36)\n",
      "|                |   hgbt |   logistic |\n",
      "|:---------------|-------:|-----------:|\n",
      "| ACC Mean Rank  | 1.2222 |     1.7778 |\n",
      "| ACC Mean Score | 0.8832 |     0.8543 |\n",
      "\n",
      "\n",
      "SELECT (MLP) VS (TREE) ON MY (35)\n",
      "|                |   hgbt |   logistic |\n",
      "|:---------------|-------:|-----------:|\n",
      "| ACC Mean Rank  | 1.2571 |     1.7429 |\n",
      "| ACC Mean Score | 0.8005 |     0.7738 |\n"
     ]
    }
   ],
   "source": [
    "methods = [ \"hgbt\", \"logistic\"] # , \"resnet\", \"rf\", \"tree\", \"mlp\",]\n",
    "metrics = [\"acc\"]\n",
    "for key in ranks_df:\n",
    "    current_result = get_result(methods, metrics, ranks_df[key]['dids'], all_my_results.reset_index())\n",
    "    current_ranks = get_average_rank_table(current_result)\n",
    "    ranks_df[key]['ranks'] = current_ranks\n",
    "    print(f\"\\n\\n{key.upper().replace('_', ' ')} ({len(ranks_df[key]['dids'])})\")\n",
    "    pprint(current_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TOO EASY (MLP) VS (TREE) ON MY (36)\n",
      "|                |   hgbt |   logistic |    mlp |   resnet |     rf |   tree |\n",
      "|:---------------|-------:|-----------:|-------:|---------:|-------:|-------:|\n",
      "| ACC Mean Rank  | 2.3056 |     4.1528 | 3.0417 |   4.1389 | 2.4306 | 4.9306 |\n",
      "| ACC Mean Score | 0.8832 |     0.8543 | 0.8728 |   0.8628 | 0.8826 | 0.861  |\n",
      "\n",
      "\n",
      "SELECT (MLP) VS (TREE) ON MY (35)\n",
      "|                |   hgbt |   logistic |    mlp |   resnet |     rf |   tree |\n",
      "|:---------------|-------:|-----------:|-------:|---------:|-------:|-------:|\n",
      "| ACC Mean Rank  | 2.2571 |     3.8571 | 2.7429 |   3.4286 | 2.8    | 5.9143 |\n",
      "| ACC Mean Score | 0.8005 |     0.7738 | 0.7932 |   0.7909 | 0.7927 | 0.721  |\n"
     ]
    }
   ],
   "source": [
    "methods = [ \"hgbt\", \"logistic\", \"resnet\", \"rf\", \"tree\", \"mlp\",]\n",
    "metrics = [\"acc\"]\n",
    "for key in ranks_df:\n",
    "    current_result = get_result(methods, metrics, ranks_df[key]['dids'], all_my_results.reset_index())\n",
    "    current_ranks = get_average_rank_table(current_result)\n",
    "    ranks_df[key]['ranks'] = current_ranks\n",
    "    print(f\"\\n\\n{key.upper().replace('_', ' ')} ({len(ranks_df[key]['dids'])})\")\n",
    "    pprint(current_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python run_autopytorch_random_experiment.py --dataset_id 151 --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"\n",
      "python run_autopytorch_random_experiment.py --dataset_id 293 --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"\n",
      "python run_autopytorch_random_experiment.py --dataset_id 354 --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"\n",
      "python run_autopytorch_random_experiment.py --dataset_id 722 --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"\n",
      "python run_autopytorch_random_experiment.py --dataset_id 821 --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"\n",
      "python run_autopytorch_random_experiment.py --dataset_id 993 --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"\n",
      "python run_autopytorch_random_experiment.py --dataset_id 1120 --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"\n",
      "python run_autopytorch_random_experiment.py --dataset_id 1461 --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"\n",
      "python run_autopytorch_random_experiment.py --dataset_id 1489 --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"\n",
      "python run_autopytorch_random_experiment.py --dataset_id 41150 --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"\n",
      "python run_autopytorch_random_experiment.py --dataset_id 42769 --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"\n",
      "python run_autopytorch_random_experiment.py --dataset_id 1044 --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"\n",
      "python run_autopytorch_random_experiment.py --dataset_id 41168 --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"\n"
     ]
    }
   ],
   "source": [
    "for dataset_id in select_hgbt_linear_dids_their:\n",
    "    print(f'python run_autopytorch_random_experiment.py --dataset_id {dataset_id} --max_configs 400 --epochs 105 --device \"cuda\" --slurm --partition alldlc_gpu-rtx2080 --nr_workers 10 --slurm_job_time_secs 10000 --exp_dir \"/work/dlclarge2/rkohli-results_tab-bench/autopytorch_cocktails_random_runs\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('tab_bench-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdd89d8741d58698331340ea54180aeeb25ca9f979bf53364921e9917b5aca39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
