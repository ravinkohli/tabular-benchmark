{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.reproduce_utils import Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(methods, metrics, dataset_names, result_df):\n",
    "    headers = [\"metric\", \"dataset\"]\n",
    "    indices = [\n",
    "        \"method\",\n",
    "        \"seed\",\n",
    "    ]\n",
    "    columns = pd.MultiIndex.from_product([metrics, dataset_names], names=headers)\n",
    "    index = pd.MultiIndex.from_product([methods, [1]], names=indices)\n",
    "    df = pd.DataFrame(columns=columns, index=index)\n",
    "    df.sort_index(inplace=True)\n",
    "    for index, row in result_df.iterrows():\n",
    "        for method in methods:\n",
    "            if int(row[\"dataset_id\"]) not in dataset_names:\n",
    "                continue\n",
    "            if \"logistic\" in method:\n",
    "                score_method = \"linear\"\n",
    "            else:\n",
    "                score_method = method\n",
    "            row_id = (method, 1)\n",
    "            col = (\"acc\", row[\"dataset_id\"])\n",
    "            df.loc[row_id, col] = row[f\"score_{score_method}\"]\n",
    "    return Results(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_rank_table(results: Results):\n",
    "    datasets = results.datasets\n",
    "    metrics = sorted(results.metrics, reverse=True)\n",
    "    # print(results.methods)\n",
    "    df = results.df\n",
    "    results_rank = {}\n",
    "    results_score = {}\n",
    "    for metric in metrics:\n",
    "        if \"time\" in metric:\n",
    "            continue\n",
    "        metric_df = df[metric]\n",
    "        dataset_rank_dfs = []\n",
    "        dataset_mean_dfs = []\n",
    "        for dataset in datasets:\n",
    "            if dataset not in metric_df.columns:\n",
    "                continue\n",
    "            dataset_rank_df = metric_df[dataset].groupby('method').mean().rank(ascending=False)\n",
    "            dataset_rank_dfs.append(dataset_rank_df)\n",
    "            dataset_mean_dfs.append(metric_df[dataset])\n",
    "\n",
    "        results_rank[metric] = pd.concat(dataset_rank_dfs).groupby(\"method\").mean()\n",
    "        \n",
    "        results_score[metric] = pd.concat(dataset_mean_dfs).groupby(\"method\").mean()\n",
    "    score_df = pd.DataFrame(results_score).reset_index()\n",
    "    rank_df = pd.DataFrame(results_rank).reset_index()\n",
    "    final_table = rank_df.merge(score_df, on=\"method\", suffixes=[\" Mean Rank\", \" Mean Score\"]).T\n",
    "    final_table.columns = final_table.iloc[0]\n",
    "    final_table = final_table.iloc[1:]\n",
    "    return final_table\n",
    "\n",
    "def pprint(df):\n",
    "    for column in df:\n",
    "        df[column] = df[column].astype('float').round(decimals=4)\n",
    "\n",
    "    print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"rf\", \"tree\", \"mlp\", \"hgbt\", \"logistic\"]\n",
    "metrics = [\"acc\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_easy_result_df = pd.read_csv(\"too_easy_without_resnet.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "too_easy_dataset_names = too_easy_result_df[\"dataset_id\"].astype(int).copy().to_list()\n",
    "too_easy_result = get_result(methods, metrics, too_easy_dataset_names, too_easy_result_df)\n",
    "too_easy_all_datasets_ranks = get_average_rank_table(too_easy_result)\n",
    "\n",
    "selected_result_df = pd.read_csv(\"benchmark_without_resnet.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "selected_dataset_names = selected_result_df[\"dataset_id\"].astype(int).copy().to_list()\n",
    "selected_result = get_result(methods, metrics, selected_dataset_names, selected_result_df)\n",
    "selected_all_datasets_ranks = get_average_rank_table(selected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Too easy (65 datasets from latest csv)\n",
      "|                |   hgbt |   logistic |    mlp |     rf |   tree |\n",
      "|:---------------|-------:|-----------:|-------:|-------:|-------:|\n",
      "| acc Mean Rank  | 2.1231 |     3.0769 | 2.6308 | 2.5154 | 4.6538 |\n",
      "| acc Mean Score | 0.8312 |     0.8194 | 0.825  | 0.8256 | 0.7783 |\n",
      "\n",
      "\n",
      "\n",
      "grinzstjan et al\n",
      "|                |   hgbt |   logistic |    mlp |     rf |   tree |\n",
      "|:---------------|-------:|-----------:|-------:|-------:|-------:|\n",
      "| acc Mean Rank  | 1.5238 |     4.6667 | 3.1429 | 1.619  | 4.0476 |\n",
      "| acc Mean Score | 0.8025 |     0.7167 | 0.7714 | 0.8045 | 0.7478 |\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\\n\\nToo easy ({len(too_easy_dataset_names)} datasets from latest csv)\")\n",
    "pprint(too_easy_all_datasets_ranks)\n",
    "print(\"\\n\\n\\ngrinzstjan et al\")\n",
    "pprint(selected_all_datasets_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Too easy (65 datasets from latest csv) + grinzstjan et al\n",
      "|                |   hgbt |   logistic |    mlp |     rf |   tree |\n",
      "|:---------------|-------:|-----------:|-------:|-------:|-------:|\n",
      "| acc Mean Rank  | 1.9767 |     3.4651 | 2.7558 | 2.2965 | 4.5058 |\n",
      "| acc Mean Score | 0.8242 |     0.7943 | 0.8119 | 0.8205 | 0.7708 |\n"
     ]
    }
   ],
   "source": [
    "all_result = Results(pd.concat([too_easy_result.df, selected_result.df]))\n",
    "all_result_ranks = get_average_rank_table(all_result)\n",
    "print(f\"\\nToo easy ({len(too_easy_dataset_names)} datasets from latest csv) + grinzstjan et al\")\n",
    "pprint(all_result_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rkohli/tabular-benchmark/result_plotting.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkisbat1/home/rkohli/tabular-benchmark/result_plotting.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m too_easy_result_df\u001b[39m.\u001b[39;49mto_csv(\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tab_bench-env/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tab_bench-env/lib/python3.8/site-packages/pandas/core/generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3709\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[1;32m   3711\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3712\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[1;32m   3713\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3717\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[1;32m   3718\u001b[0m )\n\u001b[0;32m-> 3720\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[1;32m   3721\u001b[0m     path_or_buf,\n\u001b[1;32m   3722\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[1;32m   3723\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m   3724\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   3725\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   3726\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3727\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m   3728\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   3729\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m   3730\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   3731\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m   3732\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[1;32m   3733\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   3734\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[1;32m   3735\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[1;32m   3736\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3737\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tab_bench-env/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tab_bench-env/lib/python3.8/site-packages/pandas/io/formats/format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1168\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1171\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1172\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[1;32m   1188\u001b[0m )\n\u001b[0;32m-> 1189\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1191\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/anaconda3/envs/tab_bench-env/lib/python3.8/site-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[1;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[1;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[1;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/anaconda3/envs/tab_bench-env/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "too_easy_result_df.to_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Datasets tabular data benchmark - Feuille 12.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_easy_result_df = pd.read_csv(\"too_easy_without_resnet.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "too_easy_dataset_names = too_easy_result_df[\"dataset_id\"].copy().to_list()\n",
    "too_easy_result = get_result(methods, metrics, too_easy_dataset_names, too_easy_result_df)\n",
    "too_easy_all_datasets_ranks = get_average_rank_table(too_easy_result)\n",
    "too_easy_dids = []\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        if not pd.isnull(row[\"dataset_id\"]) and row[\"Remove\"] != 1 and (row[\"too_easy\"] == 1 or args.all) and row[\"Redundant\"] != 1:\n",
    "            prefix_to_skip = [\"BNG\", \"RandomRBF\", \"GTSRB\", \"CovPokElec\", \"PCam\"]\n",
    "            if not (np.any([row[\"dataset_name\"].startswith(prefix) for prefix in\n",
    "                            prefix_to_skip]) or \"mnist\" in row[\"dataset_name\"].lower() or \"image\" in row[\n",
    "                        \"dataset_name\"].lower() or \"cifar\" in row[\"dataset_name\"].lower() or row[\"dataset_id\"] == 1414):\n",
    "                        too_easy_dids.append(int(row[\"dataset_id\"]))\n",
    "    except:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(too_easy_dids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(too_easy_dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_datasets_info = pd.read_csv(\"Datasets tabular data benchmark - categorical_classif-2.csv\").fillna(0)\n",
    "\n",
    "real_too_easy_dataset_ids = []\n",
    "for index, row in full_datasets_info.iterrows():\n",
    "    if int(row[\"too_easy\"]) and int(row[\"dataset_id\"]) != 0:\n",
    "        real_too_easy_dataset_ids.append(int(row[\"dataset_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24,\n",
       " 26,\n",
       " 154,\n",
       " 179,\n",
       " 274,\n",
       " 350,\n",
       " 720,\n",
       " 881,\n",
       " 923,\n",
       " 959,\n",
       " 981,\n",
       " 993,\n",
       " 1110,\n",
       " 1112,\n",
       " 1113,\n",
       " 1119,\n",
       " 1169,\n",
       " 1240,\n",
       " 1461,\n",
       " 1486,\n",
       " 1503,\n",
       " 1568,\n",
       " 1590,\n",
       " 4534,\n",
       " 4541,\n",
       " 40517,\n",
       " 40672,\n",
       " 40997,\n",
       " 40998,\n",
       " 41000,\n",
       " 41002,\n",
       " 41003,\n",
       " 41006,\n",
       " 41147,\n",
       " 41162,\n",
       " 41440,\n",
       " 41672,\n",
       " 42132,\n",
       " 42192,\n",
       " 42193,\n",
       " 42206,\n",
       " 42343,\n",
       " 42344,\n",
       " 42345,\n",
       " 42477,\n",
       " 42493,\n",
       " 42732,\n",
       " 42734,\n",
       " 42742,\n",
       " 42746,\n",
       " 42750,\n",
       " 43044,\n",
       " 43439,\n",
       " 43489,\n",
       " 43607,\n",
       " 43890,\n",
       " 43892,\n",
       " 43898,\n",
       " 43903,\n",
       " 43904,\n",
       " 43920,\n",
       " 43922,\n",
       " 43923,\n",
       " 43938]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_too_easy_dataset_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44156\n",
      "44157\n",
      "44159\n",
      "44160\n",
      "44161\n",
      "44162\n",
      "44186\n"
     ]
    }
   ],
   "source": [
    "import openml\n",
    "#openml.config.apikey = 'FILL_IN_OPENML_API_KEY'  # set the OpenML Api Key\n",
    "# SUITE_ID = 297 # Regression on numerical features\n",
    "# SUITE_ID = 298 # Classification on numerical features\n",
    "# SUITE_ID = 299 # Regression on numerical and categorical features\n",
    "SUITE_ID = 304 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "for task_id in benchmark_suite.tasks:  # iterate over all tasks\n",
    "    task = openml.tasks.get_task(task_id, download_data=False)  # download the OpenML task\n",
    "    # dataset = task.dataset_id(download_data=False)\n",
    "    print(task.dataset_id)\n",
    "    # X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "    #     dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('tab_bench-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdd89d8741d58698331340ea54180aeeb25ca9f979bf53364921e9917b5aca39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
